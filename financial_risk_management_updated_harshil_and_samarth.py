# -*- coding: utf-8 -*-
"""Financial risk management updated Harshil And Samarth.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AXnUvclWeRQlKZ1oZHdO58DxItMdNE3N
"""

import pandas as pd
import numpy as np                   #Importing the required libraries fr the project
import matplotlib.pyplot as plt
import joblib

df=pd.read_csv('german_credit_data.csv') #Importing th dataset from local drive
df

dataset=pd.read_csv('german_credit_data.csv')

df.columns

cols=['Sex','Housing', 'Saving accounts', 'Checking account', 'Purpose']
for i in cols:
    print(df[i].value_counts())                                           #Checking for types of values in the Dataset columns

"""Data Preprocessing"""

#checking for null values

df.describe()

df.info()

df.isnull().any()  #Checking for null values

import seaborn as sns

sns.pairplot(df)

df_encoded = df.copy()
for col in df_encoded.select_dtypes(include=['object']).columns:
    df_encoded[col] = df_encoded[col].astype('category').cat.codes

# Now compute correlation
corr = df_encoded.corr()

plt.figure(figsize=(10,6))
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.show()

sns.distplot(df['Duration'].dropna())

df['Saving accounts'].fillna(df['Saving accounts'].mode().iloc[0],inplace=True)  #Taking care of categorical null values in Saving accounts
df

df['Checking account'].fillna(df['Checking account'].mode().iloc[0],inplace=True)#Taking care of categorical null values in Checking account

df.isnull().any() #Verifying removal of null data

df['Job'].value_counts()

df['Housing'].value_counts()

df['Saving accounts'].value_counts()

df['Purpose'].value_counts()

df['Checking account'].value_counts()

"""Taking care of categorical data by Encoding"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()



df["Risk"]=lb.fit_transform(df["Risk"])

df

ct=ColumnTransformer([("on",OneHotEncoder(),[1,3,4,5,8])],remainder='passthrough')

df=ct.fit_transform(df)
df

joblib.dump(ct,'onehot.save') # Saving the column transformation

df.shape

"""Now checking for any outlier"""

from scipy import stats

z=np.abs(stats.zscore(df))
z

threshold=3
np.where(z>threshold)

"""Removing outliers"""

df_no_outliers=df[(z<=3).all(axis=1)]
df_no_outliers

x=df[:,:-1]
x

y=df[:,-1]
y

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()
x=sc.fit_transform(x)
x

joblib.dump(sc,'Scalar.save') #Saving Scalar transformation

"""Splitting into test and train datasets"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.33,random_state=45)

"""# Applying different Classification Algorithms

Logistic Regression Classification Algorithm
"""

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression()

lr.fit(x_train,y_train)

y_pred=lr.predict(x_test)
y_pred

y_test

from sklearn.metrics import accuracy_score
lr_accuracy=accuracy_score(y_test,y_pred)
lr_accuracy



"""KNN Clasisfication Algorithm"""

from sklearn.model_selection import GridSearchCV

from sklearn.neighbors import KNeighborsClassifier

KNeighborsClassifier()

knn_grid=GridSearchCV(estimator=KNeighborsClassifier(),param_grid={'n_neighbors':np.arange(1,20)},cv=5)

knn_grid.fit(x_train,y_train)

knn_grid.best_params_

knn=KNeighborsClassifier(n_neighbors=15)
knn.fit(x_train,y_train)

y_pred=knn.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score
knn_accuracy=accuracy_score(y_test,y_pred)
knn_accuracy



"""Decision Tree Regression"""

from sklearn.tree import DecisionTreeClassifier

dt_grid= GridSearchCV(estimator=DecisionTreeClassifier(),param_grid={'criterion':['gini','entropy'],'max_depth':np.arange(2,7)},cv=5)

dt_grid.fit(x_train,y_train)

dt_grid.best_params_

dt=DecisionTreeClassifier(criterion='gini',max_depth=2)

dt.fit(x_train,y_train)

y_pred=dt.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score
dt_accuracy=accuracy_score(y_test,y_pred)
dt_accuracy

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data=StringIO()
export_graphviz(dt,out_file=dot_data,filled=True,rounded=True,special_characters=True)
graph=pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(graph.create_png())



"""Naive Bayes"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import GaussianNB

pipe=Pipeline([("mn",MinMaxScaler()),("naive",GaussianNB())])

pipe.fit(x_train,y_train)

y_pred=pipe.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score
naive_accuracy=accuracy_score(y_test,y_pred)
naive_accuracy



"""Random Forest"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier()

rf_grid=GridSearchCV(estimator=RandomForestClassifier(),param_grid={'n_estimators':np.arange(1,50),'criterion':['gini','entropy'],'max_depth':np.arange(2,10)},cv=5)

rf_grid.fit(x_train,y_train)

rf_grid.best_params_

rf=RandomForestClassifier(criterion='gini',max_depth=6,n_estimators= 37)

rf.fit(x_train,y_train)

y_pred=rf.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score
rf_accuracy=accuracy_score(y_test,y_pred)   #Accuracy is highest in random forest classification therefore we are utilising it in model making
rf_accuracy

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred)
cm

import sklearn.metrics as metrics

fpr,tpr,threshold=metrics.roc_curve(y_test,y_pred)

roc_auc=metrics.auc(fpr,tpr)
roc_auc

plt.plot(fpr,tpr,label='AUC=%0.2f'%roc_auc,color='r')
plt.legend()

joblib.dump(rf,'model.save')

z=[lr_accuracy,knn_accuracy,dt_accuracy,naive_accuracy,rf_accuracy]
label=["LR","KNN","DT","Naive","RF"]
A=plt.bar(label,z,width=0.5,color=['g','r','b','y','orange'])         #Graphical representation of accuracies of differents Classification algorithms
plt.xlabel('Classification Algorithms')
plt.ylabel('Accuracy')

df.shape



clean_df=pd.DataFrame(df)
clean_df

